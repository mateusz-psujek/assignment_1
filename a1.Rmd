---
title: "Assignment 1"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pacman::p_load(tidyverse, brms, tidybayes, rstan, conflicted)


conflict_scout()
conflict_prefer('ar', 'brms')
conflict_prefer('filter', 'dplyr')
conflict_prefer('lag', 'dplyr')

```
# Part 1 - Simulating data

Before we even think of analyzing the data, we should make sure we understand the problem, and we plan the analysis. To do so, we need to simulate data and analyze the simulated data (where we know the ground truth).

In particular, let's imagine we have n autistic and n neurotypical children. We are simulating their average utterance length (Mean Length of Utterance or MLU) in terms of words, starting at Visit 1 and all the way to Visit 6.
In other words, we need to define a few parameters:
- average MLU for ASD (population mean) at Visit 1 and average individual deviation from that (population standard deviation)
- average MLU for TD (population mean) at Visit 1 and average individual deviation from that (population standard deviation)
- average change in MLU by visit for ASD (population mean) and average individual deviation from that (population standard deviation)
- average change in MLU by visit for TD (population mean) and average individual deviation from that (population standard deviation)
- an error term. Errors could be due to measurement, sampling, all sorts of noise. 

Note that this makes a few assumptions: population means are exact values; change by visit is linear (the same between visit 1 and 2 as between visit 5 and 6). This is fine for the exercise. In real life research, you might want to vary the parameter values much more, relax those assumptions and assess how these things impact your inference.


We go through the literature and we settle for some values for these parameters:
- average MLU for ASD and TD: 1.5 (remember the populations are matched for linguistic ability at first visit)
- average individual variability in initial MLU for ASD 0.5; for TD 0.3 (remember ASD tends to be more heterogeneous)
- average change in MLU for ASD: 0.4; for TD 0.6 (ASD is supposed to develop less)
- average individual variability in change for ASD 0.4; for TD 0.2 (remember ASD tends to be more heterogeneous)
- error is identified as 0.2

This would mean that on average the difference between ASD and TD participants is 0 at visit 1, 0.2 at visit 2, 0.4 at visit 3, 0.6 at visit 4, 0.8 at visit 5 and 1 at visit 6.

With these values in mind, simulate data, plot the data (to check everything is alright); and set up an analysis pipeline.
Remember the usual bayesian workflow:
- define the formula
- define the prior
- prior predictive checks
- fit the model
- model quality checks: traceplots, divergences, rhat, effective samples
- model quality checks: posterior predictive checks, prior-posterior update checks
- model comparison

Once the pipeline is in place, loop through different sample sizes to assess how much data you would need to collect. N.B. for inspiration on how to set this up, check the tutorials by Kurz that are linked in the syllabus.

BONUS questions for Part 1: what if the difference between ASD and TD was 0? how big of a sample size would you need? What about different effect sizes, and different error terms?

## Simulating the data

##### Plotting the real MLU data to see how real MLU data looks more or less
```{r}
real_data <- read_csv('a0/a0_data.csv')

gg_real_mlu <- ggplot(real_data, aes(x = chi_mlu, fill = diagnosis))+
  geom_density(alpha = 0.7)+
  facet_grid(~ visit) +
  ggtitle('Real data')

gg_real_mlu
```

##### Describing / explaining the simulation process

Given MLU ($X_{MLU}$) is log-normally distributed, it is described to be:

$$
X_{MLU} = e^{\mu +\sigma Z}
$$
Where $Z$ is the standard normal distribution.


The $\mu$ and $\sigma$ parameters here are however "**the expected value (or mean) and standard deviation of the variable's natural logarithm, not the expectation and standard deviation of $X_{MLU}$ itself**" - paraphrasing and quoting from: https://en.wikipedia.org/wiki/Log-normal_distribution#Generation_and_parameters

To obtain these parameters given the desired $\mu$ and $\sigma^2$ of the variable ($X_MLU$) itself we used these two functions(copy-pasted from the same article):

$$
\mu =\ln \left({\frac {\mu _{MLU}^{2}}{\sqrt {\mu _{MLU}^{2}+\sigma _{MLU}^{2}}}}\right)
$$

and
$$
\sigma ^{2}=\ln \left(1+{\frac {\sigma _{MLU}^{2}}{\mu _{MLU}^{2}}}\right)
$$
Where:
    $\mu_{MLU}$ is equal to $\text{indiviudal intercept} + \text{individual slope}*(\text{visit} -1)$
  and
    $\sigma^2_{MLU}$ is equal to $0.2$- i.e the measurment error


##### Creating a data simulating function
```{r}
log_mu <- function(mu, sigma){log(mu^2/sqrt(sigma^2 + mu^2))}
log_sigma <- function(mu, sigma){sqrt(log(1 + sigma^2/mu^2))} 
```
```{r}

simulate_data <- function(n = 200, 
                          seed,
                          intercept_mu_asd = 1.5, intercept_sd_asd = 0.5,
                          intercept_mu_td = 1.5, intercept_sd_td = 0.5,
                          beta_mu_asd = 0.4, beta_sd_asd = 0.4,
                          beta_mu_td = 0.6, beta_sd_td = 0.2,
                          error = 0.2
                          ){
  
  set.seed = seed
  
    tibble(id = 1:n,
         diagnosis = rep(c('asd', 'td'), times = n/2)
         ) %>% 
    expand_grid(visit = 1:6) %>% 
    group_by(id) %>% 
    mutate(
         ind_intercept = ifelse(
           diagnosis == 'asd',
           rnorm(1, intercept_mu_asd, intercept_sd_asd),
           rnorm(1, intercept_mu_td, intercept_sd_td)
         ),
         ind_slope = if_else(
           diagnosis == 'asd',
           rnorm(1, beta_mu_asd, beta_sd_asd),
           rnorm(1, beta_mu_td, beta_sd_td)
         )
    ) %>% 
    rowwise %>% 
    mutate(
      mlu = rnorm(
        n = 1,
        mean = log_mu(
          mu = ind_intercept + ind_slope * (visit - 1),
          sigma = error
          ),
        sd = log_sigma(
          mu = ind_intercept + ind_slope * (visit - 1),
          sigma = error
        )
      ),
      
      mlu = mlu %>% exp
    )
}
```
##### Checking if everything worked fine
```{r}
sim_data <- simulate_data(seed = 123)

head(sim_data)
tail(sim_data)

```

```{r}
#making a very large dataset to make sure any potential errors aren't due too random noise
check <- simulate_data(n = 10000, seed = 123)


check %>% 
  group_by(diagnosis) %>% 
  summarise(across(starts_with('ind'), list(mean = mean, sd = sd), .names = '{.col}_{.fn}')) %>% 
  mutate(across(where(is.numeric), ~ .x %>% round(3)))



# "This would mean that on average the difference between ASD and TD participants is 0 at visit 1, 0.2 at visit 2, 0.4 at visit 3, 0.6 at visit 4, 0.8 at visit 5 and 1 at visit 6."
check %>% 
  group_by(visit) %>% 
  summarise(diff = (mean(mlu[diagnosis == 'asd']) - mean(mlu[diagnosis == 'td'])) %>% round(3))

#checking the differences between the groups
#beta_mu_asd = 0.4 and beta_mu_td = 0.4

check %>% 
  group_by(diagnosis, visit) %>%
  summarise(mean_mlu = mean(mlu) %>% round(3)) %>% 
      mutate(diff_mlu = mean_mlu - lag(mean_mlu))
```



```{r}
# comparing the simulated data to some real mlu data
gg_real_mlu

ggplot(check)+
  geom_density(aes(mlu, fill = diagnosis), alpha = 0.7)+
  facet_grid(~ visit) +
  ggtitle('Simulated data, n = 2000')

ggplot(sim_data)+
  geom_density(aes(mlu, fill = diagnosis),  alpha = 0.7)+
  facet_grid(~ visit) +
  ggtitle('Simulated data, n = 100')

rm(check)
```
## Analysing the data
Remember the usual bayesian workflow:
- define the formula
- define the prior
- prior predictive checks
- fit the model
- model quality checks: traceplots, divergences, rhat, effective samples
- model quality checks: posterior predictive checks, prior-posterior update checks
- model comparison

##### Defining the formula
```{r}
# Individual intercepts and slopes for each participant, pooling done without regard to the participant's condition (diagnosis)
f <- bf(mlu ~ 0 + diagnosis + diagnosis:visit + (1 + visit|id))


# Individual intercepts and slopes for each participant, pooling done taking into account participants coming from two diffrent groups (diagnosis)
fg <- bf(mlu ~ 0 + diagnosis + diagnosis:visit + (1 + visit | gr(id, by = diagnosis)))

```
##### Define the priors:
```{r}
get_prior(formula = f,
          data = sim_data,
          family = lognormal)

get_prior(fg,
          data = sim_data,
          family = lognormal)
```


```{r}
head(p)

priors <- c(
  prior(normal(0, 0.2), class = b),
  prior(normal(0.4, 0.1), class = b, coef = diagnosisasd),
  prior(normal(0.4, 0.1), class = b, coef = diagnosistd),
  prior(normal(0, 0.2), class = sd, coef = Intercept, group = id),
  prior(normal(0, 0.1), class = sd, coef = visit, group = id)
        )



```

```{r}
f_m_prior <- brm(
  f, 
  data = sim_data,
  family = lognormal,
  prior = priors,
  sample_prior = 'only',
  backend = 'cmdstanr',
  cores = 3,
  control = list(
    adapt_delta = 0.99,
    max_treedepth = 20
  )
)
```
##### Prior predictive check:
```{r}
summary(f_m_prior)
pp_check(f_m_prior, ndraws = 100)
```
##### Fitting the model
```{r}
f_m <- brm(
  f, 
  data = sim_data,
  family = lognormal,
  prior = priors,
  sample_prior = TRUE,
  backend = 'cmdstanr',
  cores = 3,
  control = list(
    adapt_delta = 0.99,
    max_treedepth = 20
  )
)


fg_m <- brm(
  fg,
  data = sim_data,
  family = lognormal,
  prior = priors,
  sample_prior = T,
  backend = 'cmdstanr',
  cores = 3,
  control = list(
    adapt_delta = 0.99,
    max_treedepth = 20
  )
)
```
##### Model quality checks:
```{r}
summary(f_m)
summary(fg_m)
```

##### Checking convergance:
```{r}
models <- list(f_m, fg_m)
      
# launch_shinystan(f_m) # - very nice for exploring and diagnosing the model, but opens up in a new window
map(.x = models, ~ mcmc_plot(.x, type = 'trace') + 
    theme_classic() + 
    scale_color_manual(values=c("#E66101", "#998EC3", "#542788", "#F1A340")) + 
    ylab("") + 
    xlab("Iteration") + 
    labs(subtitle = 'Trace Plots'))

map(.x = models, ~ mcmc_plot(.x, type = 'rhat_hist'))
map(.x = models, ~ mcmc_plot(.x, type = 'neff'))
```

##### Posterior predictive checks:
```{r}
pp_check(f_m, ndraws = 100)
pp_check(fg_m, ndraws = 100)
```




##### Prior-posterior updata checks:
```{r}
i <- get_variables(fg_m)
i[grepl("prior_sd", i, fixed = TRUE)]

```
##### Note:
For some reason, in the case of varying slopes by diagnosis the prior function seems to automatically only set the standard deviation priors for the 'ASD' diagnosis and not the 'TD' diagnosis.
From what we managed to find only the possibility of setting the priors separately for each group in gr(by = x) might be an open issue in brms (https://github.com/paul-buerkner/brms/issues/874). Because of that we only made prior - posterior plots for the 'ASD' group.
```{r}

pp_update_plot <- function(model){

df <- left_join(spread_draws(model, `.*b_.*`, regex = TRUE),
             spread_draws(model, `.*sd_.*`, regex = TRUE)) %>% 
  select(!c(.chain, .iteration, .draw) &
           !contains('cor') &
           !any_of(c("sd_id__Intercept:diagnosistd", "sd_id__visit:diagnosistd")))

gg_posteriors <- select(df, !starts_with('prior')) %>% as.list
gg_priors <- select(df, starts_with('prior')) %>% as.list
gg_titles <- names(gg_posteriors)

pmap(.l = list(.x = gg_priors, .y = gg_posteriors, .t = gg_titles),
     .f = function(.x, .y, .t){
       ggplot()+
            geom_density(aes(.x, fill = 'steelblue', alpha = 0.5))+
            geom_density(aes(.y, fill = '#FC4E07', alpha = 0.5))+
            ggtitle(.t)+
            theme_classic()+
            guides(fill = 'none', alpha = 'none') +
            labs(x = NULL)}
)}
```


```{r}
pp_update_plot(f_m)
```


```{r}
pp_update_plot(fg_m)
```
##### Sensitivity analysis:
```{r}
sensitivity_analysis<- function(sds, formula){

sensitivity_data <- tibble()
  
sensitivity_data <- map_df(
  .x = sds,
  .f = function(.x){
  
  new_priors <- priors
  new_priors[1, ] <- set_prior(paste0("normal(0, ", .x ,")"), class = "b")
    
  model <- brm(
    formula, 
    data = sim_data,
    family = lognormal,
    prior = new_priors,
    sample_prior = TRUE,
    backend = 'cmdstanr',
    cores = 3,
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20)
    )
  
  
    rbind(sensitivity_data,
          gather_draws(model, c(`b_diagnosisasd:visit`, `b_diagnosistd:visit`)) %>% 
            median_qi(.width = 0.95) %>% 
            mutate(sd = .x)
          )
  })
}
```

```{r results = 'hide'}
sds <-  seq(0.1, 1, by = 0.1)

sensitivity_data_f <- sensitivity_analysis(sds = sds, formula = f)

sensitivity_data_fg <- sensitivity_analysis(sds = sds, formula = fg)



sensitivity_data <- bind_rows(
  sensitivity_data_f %>% mutate(formula = 'f'),
  sensitivity_data_fg %>% mutate(formula = 'fg')
)

write_rds(sensitivity_data, 'rdata/sensitivity_data.Rds')

```


```{r}
sensitivity_data %>% 
  ggplot(aes(x = sd, y = .value)) +
    geom_point(size = 3) +
    geom_pointrange(aes(ymin = .lower, ymax = .upper)) +
    ylim(0.1, 0.3) +
    labs(x = "Standard Deviation of Slope Prior", 
         y = "Posterior Estimate for slope",
         subtitle = "Sensitivity analysis for multi-level model") +
    theme_bw() +
    theme(plot.title = element_text(hjust = 0.5, size = 15),
          axis.title.x = element_text(size = 13),
          axis.text.y = element_text(size = 12),
          axis.text.x = element_text(size = 12),
          axis.title.y = element_text(size = 13)) +
    facet_grid(.variable ~ formula, switch = 'y')
```
##### Model exploration
##### Hypothesis testing
```{r}
conditional_effects(f_m)
conditional_effects(fg_m)
```
```{r}
hypothesis(f_m, 'diagnosistd:visit - diagnosisasd:visit > 0') 
hypothesis(fg_m, 'diagnosistd:visit - diagnosisasd:visit > 0') 
```



```{r}
# comparing model estimates with the true generative process values

t_vs_est <- function(model){
est <- rbind(model %>% gather_draws(`b_.*`, regex = T) %>% mean_qi,
             model %>% gather_draws(`sd_.*`, regex = T) %>% mean_qi) %>% 
  select(!c(.point, .interval)) %>% 
  rename_with(.cols = everything(), ~ paste0("est_", .x) %>% str_remove_all(fixed('.'))) %>% 
  mutate(across(where(is.numeric), exp))

if (nrow(est) == 6){
  est <- rbind(est, est %>% slice(c(5,6)))
  } else{
    est <- est
  }

t_vs_est <- tibble(term = 
                    map2_chr(.x = rep(c('mu','sd'), each = 4),
                           .y = c(rep(c('intercept', 'slope'), times = 2), 
                                  rep(c('intercept', 'slope'), each = 2)),
                           .f = ~ paste0(.x, '_', .y)),
                   diagnosis = c('asd', 'asd', 'td', 'td', 'asd', 'td', 'asd', 'td'),
                   t_value = c(1.5, 0.4, 1.5, 0.6, 0.5, 0.4, 0.5, 0.2),
                   est)

map(.x = t_vs_est %>% group_split(term),
    .f = ~ 
      ggplot(.x, aes(y = est_value, x = diagnosis, color = diagnosis)) +
        geom_hline(aes(yintercept = t_value, color = diagnosis), linetype = 'dashed') +
        geom_point(size = 3) +
        geom_pointrange(aes(ymin = est_lower, ymax = est_upper)) +
        labs(title = paste0("'", .x$term, "'"),
             subtitle = "True value vs. model estimate",
             y = "True value",
             x = "Estimated value by diagnosis") +
        theme_bw() +
        theme(axis.text.x = element_blank())
    )
}
```


```{r}
t_vs_est(f_m)
```
```{r}
t_vs_est(fg_m)
```
##### individual effects

```{r}

ind_t_vs_est <- function(model){

t_value <- sim_data %>% ungroup %>% filter(visit == 1)
t_value <- rbind(
    t_value %>% select(ind_int) %>% rename('t_value' = ind_int),
    t_value %>% select(ind_slope) %>% rename('t_value' = ind_slope)
  )

ind_t_vs_est <- model %>% recover_types(sim_data) %>% gather_draws(r_id[id, term]) %>%
  mean_qi(.width = 0.95) %>% 
  select(!c(.variable, .point, .interval, .width)) %>% 
  rename_with(.cols = !c(term, id), ~ paste0("est_", .x) %>% str_remove_all(fixed('.')))%>% 
  mutate(across(where(is.numeric), exp)) %>% 
  cbind(t_value) %>% 
  mutate(accuracy = ifelse(t_value < est_upper & t_value > est_lower, 'accurate', 'inaccurate'),
         diagnosis = rep(c('asd', 'td'), each = 200)) 

map(.x = ind_t_vs_est %>% group_split(term),
    .f = ~
      ggplot(.x, aes(x = as.numeric(id), y = est_value, color = accuracy)) +
        geom_point(aes(shape = diagnosis), size = 3) +
        geom_pointrange(aes(ymin = est_lower, ymax = est_upper)) +
        scale_color_manual(values=c('green', 'red')) + 
        labs(title = 'True individual effects vs. model estimates',
             subtitle = paste0("'", .x$term, "'"),
             x = 'ID',
             y = 'Estimated value with 95% CI')
  )
}
        
```
```{r}
ind_t_vs_est(f_m)
```
```{r}
ind_t_vs_est(fg_m)
```
## Power Analysis
```{r}
update_fg <- function(n, seed){
  df <- 
    update(fg_m,
       newdata = simulate_data(n = n, seed = seed),
       seed = seed, 
       iter = 1000) %>% 
    gather_draws(c(`b_diagnosisasd:visit`,`b_diagnosistd:visit`)) %>%
    mean_qi(.width = 0.95) %>% 
    rename_with(.cols = everything(), ~ str_remove_all(.x, fixed('.'))) %>% 
    mutate(across(where(is.numeric), function(.x){ .x= lead(.x) - .x}),
           variable = 'b_diff') %>% 
    na.omit
}
```


```{r}
power_data <- expand_grid(N = seq(10, 110, by = 20), seed = 1:50) %>% 
   mutate(dfs = update_fg(seed, N)) %>% 
   unnest(dfs)

write_rds(power_data, 'rdata/power_data.Rds')
save.image('rdata/a1_part1.Rdata') 
```
##### NHST type 'significance' approach:
```{r}
power_data %>% 
 ggplot(aes(x = seed %>% reorder(lower), y = value, ymin = lower, ymax = upper)) +
          geom_hline(yintercept = c(0, 0.2), color = "white") +
          geom_pointrange(fatten = 1/2) +
          labs(title = "Difference of mean slope parameters between the two groups",
               subtitle = paste0("N = ",  power_data$N),
               x = "seed (i.e., simulation index)",
               y = "value")+
          facet_grid(~ N)


power_data %>%
  mutate(signif = if_else(lower*upper > 0, 1, 0)) %>% 
  group_by(N) %>% 
  summarise(
    p_signif = mean(signif),
    se = sqrt(p_signif*(1 - p_signif) / n())
         ) %>% 
  mutate(lower.95 = ymin = p_signif - 1.96*se, 
         upper.95 = p_signif + 1.96*se) %>% 
  ggplot(aes(x = N, y = p_signif, ymin = lower.95, ymax = upper.95)) +
    geom_pointrange() +
    geom_line()
  
  
```
##### Accuracy in parameter estimation (precision) approach :

```{r}
power_data %>% 
  group_by(N, seed) %>% 
  mutate(ci_width = upper - lower) %>% 
  ungroup %>% 
  ggplot(aes(x = ci_width)) %>% 
  geom_bar(binwidth = 0.1)


powerdata%>% 
  mutate(check = ifelse(ci_width < 0.7, 1, 0)) %>% 
  summarise(`proportion below 0.7` = mean(check),
            `average width` = mean(ci_width))


power_data %>%
  mutate(precise = if_else(width < 0.7, 1, 0)) %>% 
  group_by(N) %>% 
  summarise(
    p_precise = mean(precise),
    se = sqrt(p_precise*(1 - p_precise) / n())
         ) %>% 
  mutate(lower.95 = ymin = p_precise - 1.96*se, 
         upper.95 = p_precise + 1.96*se) %>% 
  ggplot(aes(x = N, y = p_precise, ymin = lower.95, ymax = upper.95)) +
    geom_pointrange() +
    geom_line()
  
```
## Conclusions:
- moved to the report file





# Part 2 - Strong in the Bayesian ken, you are now ready to analyse the actual data

- Describe your sample (n, age, gender, clinical and cognitive features of the two groups) and critically assess whether the groups (ASD and TD) are balanced. Briefly discuss whether the data is enough given the simulations in part 1.
- Describe linguistic development (in terms of MLU over time) in TD and ASD children (as a function of group). Discuss the difference (if any) between the two groups.
- Describe individual differences in linguistic development: do all kids follow the same path? Are all kids reflected by the general trend for their group?

- Include additional predictors in your model of language development (N.B. not other indexes of child language: types and tokens, that'd be cheating). Identify the best model, by conceptual reasoning, model comparison or a mix. Report the model you choose (and name its competitors, if any) and discuss why it's the best model.

##### Cleaning the environment 
```{r}
rm(f, f_m, sim_data, f_m_prior, fg_m, gg_real_mlu, models, real_data, p, sim_data, i, log_mu, log_sd, simulate_data)
```


##### loading the data
```{r}
data <- read_csv('data_clean.csv') %>%
  rename_with(.cols = everything(), ~ str_to_lower(.x)) %>%
  rename(id = child.id, mlu = chi_mlu) %>% 
  mutate(across(where(is.character), str_to_lower))
```
## Describing the samples
```{r}
data <- data %>% 
  mutate(
    ethnicity = str_replace_all(ethnicity, c('bangledeshi' = 'asian', 
                                            'bangladeshi' ='asian', 
                                            'white American' = 'white')) %>%
                replace(ethnicity == 'latino' | ethnicity == 'hispanic', 'latino/hispanic'))
# there was some messy data in the ethnicity column (misspellings, different naming schemes)

data_s <- data %>% filter(visit == 1)


#sizes of the two conditions
data_s %>% count(diagnosis)

gg <- ggplot(data_s, aes(x = diagnosis, fill = diagnosis)) + theme_minimal()
gg + geom_bar()

#gender
data_s %>% count(diagnosis, gender) 

data %>% 
 ggplot(aes(diagnosis, fill = gender))+
 geom_bar( )+
 theme_minimal() +
 ggtitle('Gender across conditions')

# ethnicity and gender
data_s %>% count(diagnosis, gender, ethnicity)
data_s %>% count(ethnicity) %>% mutate(pct = n / (n %>% sum), pct = round(pct, 2))


# cognitive functions
data_s %>% group_by(diagnosis) %>% summarise(across(c(verbaliq1, nonverbaliq1), ~ mean(.x, na.rm = TRUE)))
  
gg + geom_violin(aes(y = verbaliq1)) + ggtitle('Verbal IQ')
gg + geom_violin(aes(y = nonverbaliq1)) + ggtitle('Non-verbal IQ')


# age
data_s %>% group_by(diagnosis) %>% summarise(age_mean = mean(age, na.rm = T), age_sd = sd(age, na.rm = T))

data %>% 
  ggplot(aes(age, fill = diagnosis))+
  geom_density(alpha = 0.6)+
  theme_minimal() +
  ggtitle('Age accross conditions')
 
# socialisation
data_s %>% group_by(diagnosis) %>% summarise(socialisation_mean = mean(socialization, na.rm = T),
                                             socialisation_sd = sd(socialization, na.rm = T))

data %>% 
 ggplot(aes(socialization, fill = diagnosis))+
 geom_density(alpha = 0.6)+
 theme_minimal() +
 ggtitle('Socialisation score acorss conditions')

rm(data_s, gg)
```
##### Conclusions:
The number of participants in each group is comparable, but the 'TD' group is slightly bigger(35 vs. 31). The gender(female xor male) distribution within both groups is similarly unequal with approx 5 times less female then male participants in each group (29 and 6 for 'TD'; 26 and 5 for 'ASD'). The sample is widely dominated by 'White' ethnicity (86% of all participants). The rest of the participants if divided between different ethnic minorities of only 1 or 2 participants each (2% to 3% of all participants). The 2 groups vary widely in terms of the ethnic diversity with only 1 participant with ethnicity different than 'White' in the TD group. The mean of both verbal and nonverbal IQ are similar in both groups. However, the ASD group shows greater variance in both parameters. In conclusion, the groups seem to be balanced (there are similar enough in measures other then their diagnosis). However, neither of the samples shouldn't be treated as representative for populations other than white males.

## Linguistic development as a function of group(condition)

##### Fitting the models
```{r}

#keeping priors from part 1 now, ill see whether i should change them later

model_prior <- brm(
  fg,
  data = data,
  family = lognormal,
  prior = priors,
  sample_prior = 'only',
  backend = 'cmdstanr',
  cores = 3,
  control = list(
    adapt_delta = 0.99,
    max_treedepth = 20
  )
)
```
##### Prior predictive checks
```{r}
pp_check(model_prior, ndraws = 100)
```

```{r}
model <- brm(
  fg,
  data = data,
  family = lognormal,
  prior = priors,
  sample_prior = T,
  backend = 'cmdstanr',
  cores = 3,
  control = list(
    adapt_delta = 0.99,
    max_treedepth = 20
  )
)
```
```{r}
summary(model)
```

##### Checking convergance:
```{r}
# launch_shinystan(f_m) # - very nice for exploring and diagnosing the model, but opens up in a new window

mcmc_plot(model, type = 'trace') + 
    theme_classic() + 
    scale_color_manual(values=c("#E66101", "#998EC3", "#542788", "#F1A340")) + 
    ylab("") + 
    xlab("Iteration") + 
    labs(subtitle = 'Trace Plots')

mcmc_plot(model, type = 'rhat_hist')
mcmc_plot(model, type = 'neff')
```
##### Posterior predictive checks:
```{r}
pp_check(model, ndraws = 100)
```
##### Prior-posterior update checks
```{r}
pp_update_plot(model)
```
### Differences between the groups
```{r}
conditional_effects(model)
```
##### Hypothesis testing
```{r}
hypothesis(model, 'diagnosistd:visit - diagnosisasd:visit > 0') 
```
##### Plotting model estimates
```{r}

est <- rbind(model %>% gather_draws(`b_.*`, regex = T) %>% mean_qi,
             model %>% gather_draws(`sd_.*`, regex = T) %>% mean_qi) %>% 
  select(!c(.point, .interval)) %>% 
  rename_with(.cols = everything(), ~ paste0("est_", .x) %>% str_remove_all(fixed('.'))) %>% 
  mutate(across(where(is.numeric), exp))


est <- tibble(term =
         map2_chr(.x = rep(c('mu','sd'), each = 4),
                  .y = c(rep(c('intercept', 'slope'), times = 2), 
                         rep(c('intercept', 'slope'), each = 2)),
                  .f = ~ paste0(.x, '_', .y)),
       diagnosis = c('asd', 'asd', 'td', 'td', 'asd', 'td', 'asd', 'td'),
       est)

map(.x = est %>% group_split(term),
    .f = ~ 
      ggplot(.x, aes(y = est_value, x = diagnosis, color = diagnosis)) +
        geom_hline(yintercept = 0, color = 'darkred', linetype = 'dashed') +
        geom_point(size = 3) +
        geom_pointrange(aes(ymin = est_lower, ymax = est_upper)) +
        labs(title = paste0("'", .x$term, "'"),
             subtitle = "True value vs. model estimate",
             y = "True value",
             x = "Estimated value by diagnosis") +
        theme_bw() +
        theme(axis.text.x = element_blank())
    )

```
### Individual differences
```{r}

```

## Including additional models
```{r}
m_baseline <- model
rm(model)

#only environmental factors
m_env <- bf(mlu ~ 0 + diagnosis + visit:Diagnosis + socialization:diagnosis + (1 + visit | id))

#only cognitive
m_cog <- bf(mlu ~ 0 + diagnosis + visit:diagnosis + verblaiq:diagnosis + nonverbalis:diagnosis + (1 + visit | id))

#environmental and cognitive factors
m_env_cog <- bf(mlu ~ 0 + diagnosis + visit:diagnosis + nonverbal + socialization + (1 + visit | id))

formulas <- list(m_env = m_env, m_cog = m_cog, m_env_cog = m_env_cog)

```


```{r}
models <- formulas %>% 
  map(
    ~ brm(
        .x,
        data = data,
        family = lognormal,
        prior = priors,
        sample_prior = 'only',
        backend = 'cmdstanr',
        cores = 3,
        control = list(
        adapt_delta = 0.99,
        max_treedepth = 20
        )
    )
  )
```
###### convergance checks
```{r}
models[-1] %>% 
  map(
    ~ diag_plots <- list(
        mcmc_plot(type = 'trace'),
        mcmc_plot(type = 'rhat_hist'),
        mcmc_plot(type = 'neff')
    )
  print(diag_plots)
  )
```
###### Posterior-predictive checks
```{r}
models[-1] %>% map( ~ pp_update_plot(.x))
```
###### Model comparison
```{r}
kfs <- models %>% map( ~ kfold(.x, folds = 'stratified', group = 'id', K = 5, save_fits = T))

kf_preds <- kfs %>% map( ~ kfold_predict(.x))

rmse <- function(y, yrep){
  yrep_mean <- colMeans(yrep)
  sqrt(mean((yrep_mean - y)^2))
}

kf_rmses <- kf_preds %>% map( ~ rmse(y = .x$y, yrep = .x$yrep))


#cross-validation performance
kf_rmses %>% 
  as.tibble %>% 
  pivot_longer(cols = everything(),
               names_to = 'model',
               values_to = 'RMSE') %>% 
  ggplot(aes(x = model, y = RMSE, colour = model)) +
    geom_point() +
    theme_minimal() +
    ggtitle("Accessing performance using cross-validation, k = 5")
```


```{r}
# we know normally it's not a good practice to access performance on the training set, but we wanted to see how it would compare

#to_do run tomorrow with actual data and see whether it works
preds <- models %>% map(~ posterior_predict(.x))
training_rmses <- preds %>% map(~ rmse(y = xdata$mlu, yrep = .x))


bind_rows(
  training_rmses %>%  
    as.tibble %>% 
    pivot_longer(cols = everything(),
                 names_to = 'model',
                 values_to = 'RMSE'
                 ) %>% 
    mutate(type = 'training'),
  kf_rmses %>% 
    as.tibble %>% 
    pivot_longer(cols = everything(),
               names_to = 'model',
               values_to = 'RMSE'
               ) %>% 
    mutate(type = 'cross-validation')
) %>% 
  ggplot(aes(x = type, y = RMSE, colour = model)) +
    geom_point() +
    geom_line() +
    theme_minimal() +
    ggtitle("Comparing performance on training dataset and using cross-validation")
```
### Conclusions:

- moved to the report file
