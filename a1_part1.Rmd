---
title: "a1_part1"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pacman::p_load(tidyverse, brms, tidybayes, rstan, conflicted)


conflict_scout()
conflict_prefer('ar', 'brms')
conflict_prefer('filter', 'dplyr')
conflict_prefer('lag', 'dplyr')
```
## Part 1 - Simulating data

Before we even think of analyzing the data, we should make sure we understand the problem, and we plan the analysis. To do so, we need to simulate data and analyze the simulated data (where we know the ground truth).

In particular, let's imagine we have n autistic and n neurotypical children. We are simulating their average utterance length (Mean Length of Utterance or MLU) in terms of words, starting at Visit 1 and all the way to Visit 6.
In other words, we need to define a few parameters:
- average MLU for ASD (population mean) at Visit 1 and average individual deviation from that (population standard deviation)
- average MLU for TD (population mean) at Visit 1 and average individual deviation from that (population standard deviation)
- average change in MLU by visit for ASD (population mean) and average individual deviation from that (population standard deviation)
- average change in MLU by visit for TD (population mean) and average individual deviation from that (population standard deviation)
- an error term. Errors could be due to measurement, sampling, all sorts of noise. 

Note that this makes a few assumptions: population means are exact values; change by visit is linear (the same between visit 1 and 2 as between visit 5 and 6). This is fine for the exercise. In real life research, you might want to vary the parameter values much more, relax those assumptions and assess how these things impact your inference.


We go through the literature and we settle for some values for these parameters:
- average MLU for ASD and TD: 1.5 (remember the populations are matched for linguistic ability at first visit)
- average individual variability in initial MLU for ASD 0.5; for TD 0.3 (remember ASD tends to be more heterogeneous)
- average change in MLU for ASD: 0.4; for TD 0.6 (ASD is supposed to develop less)
- average individual variability in change for ASD 0.4; for TD 0.2 (remember ASD tends to be more heterogeneous)
- error is identified as 0.2

This would mean that on average the difference between ASD and TD participants is 0 at visit 1, 0.2 at visit 2, 0.4 at visit 3, 0.6 at visit 4, 0.8 at visit 5 and 1 at visit 6.

With these values in mind, simulate data, plot the data (to check everything is alright); and set up an analysis pipeline.
Remember the usual bayesian workflow:
- define the formula
- define the prior
- prior predictive checks
- fit the model
- model quality checks: traceplots, divergences, rhat, effective samples
- model quality checks: posterior predictive checks, prior-posterior update checks
- model comparison

Once the pipeline is in place, loop through different sample sizes to assess how much data you would need to collect. N.B. for inspiration on how to set this up, check the tutorials by Kurz that are linked in the syllabus.

BONUS questions for Part 1: what if the difference between ASD and TD was 0? how big of a sample size would you need? What about different effect sizes, and different error terms?

## Simulating the data

### Plotting the real MLU data to see how real MLU data looks more or less
```{r}

real_data <- read_csv('a0_data.csv')

gg_real_mlu <- ggplot(real_data, aes(x = chi_mlu, fill = diagnosis, alpha = 0.3))+
  geom_density()+
  facet_wrap(~ visit)

gg_real_mlu
```
### Making a tibble with the distribution parameters
```{r}
#- average MLU for ASD and TD: 1.5 (remember the populations are matched for linguistic ability at first visit)
#- average individual variability in initial MLU for ASD 0.5; for TD 0.3 (remember ASD tends to be more heterogeneous)
#- average change in MLU for ASD: 0.4; for TD 0.6 (ASD is supposed to develop less)
#- average individual variability in change for ASD 0.4; for TD 0.2 (remember ASD tends to be more heterogeneous)
#- error is identified as 0.2

# mlu = intercept +visit*beta + error
# mlu = G(i_mu, i_sd) + G(v_mu, v_sd) + G(e_mu, e_sd)


#to_do list:
  # - might be more suitable to use something else than the normal distirubtion for the error
  # - understand the log normal distribution mathematically more
  # - understand why the prior check gets such a crazily wide scale
  # - find a way to could find the number of parameters for which the effective samples were below 0.1 of the total sample number

# Questions:
#   - why does the prior pp_check show negative predictions???
#   - how to set the sd prior for both asd and td for the gr(by = diagnosis) model (seems to be an open issue in brms <link>)

log_mu <- function(mu, sd){log(mu^2/sqrt(sd^2 + mu^2))}
log_sd <- function(mu, sd){sqrt(log(1 + sd^2/mu^2))} #taken from https://en.wikipedia.org/wiki/Log-normal_distribution#Arithmetic_moments


p <- tibble(parameter = c('intercept', 'slope', 'error') %>% rep(2),
            diagnosis = c('asd', 'td') %>% rep(each = 3) %>% as_factor,
            mu = c(1.5, 0.4, 0, 1.5, 0.6, 0),
            sd = c(0.5, 0.4, 0.2, 0.3, 0.2, 0.2)) %>%
  group_by(parameter) %>% 
  mutate(mu_log = if_else(parameter == 'error', mu, log_mu(mu, sd)),
         sd_log = if_else(parameter == 'error', sd, log_sd(mu, sd))
         )



head(p)
```
### Simulating the data
```{r}

# n - the number of participants in each condition


simulate_data <- function(seed, n){

  set.seed(seed)
  
  df <- tibble(id = seq(1, (n*2), by = 1) %>% as_factor) %>% 
    mutate(diagnosis = rep(c('asd', 'td'), each = n) %>% as_factor) %>%
    slice(rep(seq_along(id), each = 6)) %>% 
    group_by(id) %>% 
    mutate(
           ind_int = if_else(diagnosis == 'asd',
                             rlnorm(1, mean = p$mu_log[[1]], sd = p$sd_log[[1]]),
                             rlnorm(1, mean = p$mu_log[[4]], sd = p$sd_log[[4]])),
           
           ind_slope = if_else(diagnosis == 'asd',
                               rlnorm(1, mean =  p$mu_log[[2]], sd = p$sd_log[[2]]),
                               rlnorm(1, mean =  p$mu_log[[5]], sd = p$sd_log[[5]])),
           
          visit = seq(1, 6, by = 1),
           
          t_mlu = if_else(visit == 1, ind_int, ind_int + ind_slope*(as.numeric(visit) - 1)),
          
          mlu = t_mlu + rnorm(seq_along(n()), mean = p$mu[[3]], sd = p$sd[[3]])
          )
          
  return(df)
}

sim_data <- simulate_data(1, 10000) #such a number of participants is unrealistically high, but the point here is just to check whether there are any disprepancies between the data generative process described in the instructions and the one we managed to code. As well as how does data generated by the simulation compare to the real data.
```

### Checking whether all worked fine
```{r}

glimpse(sim_data)

sim_data %>% 
  group_by(diagnosis) %>% 
  summarise(across(starts_with('ind'), list(mean = mean, sd = sd), .names = '{.col}_{.fn}'))


# "This would mean that on average the difference between ASD and TD participants is 0 at visit 1, 0.2 at visit 2, 0.4 at visit 3, 0.6 at visit 4, 0.8 at visit 5 and 1 at visit 6."

sim_data %>% 
  group_by(visit) %>% 
  summarise(avg_diff = mean(t_mlu[diagnosis == 'td']) - mean(t_mlu[diagnosis == 'asd']))
```


```{r}
# plotting the resulting individual integer and slope distributions

ggplot(sim_data)+
  geom_density(aes(ind_int, fill = diagnosis, alpha = 0.2))
ggsave('ind_slope.png')

ggplot(sim_data)+
  geom_density(aes(ind_slope, fill = diagnosis, alpha = 0.2))
ggsave('ind_intercept.png')
```


```{r}
# comparing the simulated data to some real mlu data
gg_real_mlu

ggplot(sim_data)+
  geom_density(aes(t_mlu, fill = diagnosis, alpha = 0.2))+
  facet_wrap(~ visit)

ggplot(sim_data)+
  geom_density(aes(mlu, fill = diagnosis, alpha = 0.2))+
  facet_wrap(~ visit)
ggsave('plot1.png')
```
## Analysing the data
Remember the usual bayesian workflow:
- define the formula
- define the prior
- prior predictive checks
- fit the model
- model quality checks: traceplots, divergences, rhat, effective samples
- model quality checks: posterior predictive checks, prior-posterior update checks
- model comparison

### Defining the formula
```{r}
# Individual intercepts and slopes for each participant, pooling done without regard to the participant's condition (diagnosis)
f <- bf(mlu ~ 0 + diagnosis + diagnosis:visit + (1 + visit|id))


# Individual intercepts and slopes for each participant, pooling done taking into account participants coming from two diffrent groups (diagnosis)
fg <- bf(mlu ~ 0 + diagnosis + diagnosis:visit + (1 + visit | gr(id, by = diagnosis)))

```
### Define the priors:
```{r}
get_prior(formula = f,
          data = sim_data,
          family = lognormal)

get_prior(fg,
          data = sim_data,
          family = lognormal)
```


```{r}
head(p)

priors <- c(
  prior(normal(0, 0.2), class = b),
  prior(normal(0.4, 0.1), class = b, coef = diagnosisasd),
  prior(normal(0.4, 0.1), class = b, coef = diagnosistd),
  prior(normal(0, 0.2), class = sd, coef = Intercept, group = id),
  prior(normal(0, 0.1), class = sd, coef = visit, group = id)
        )



```



```{r}
# setting the number of participant lower to make it more realitic for a study setting and decrease the computation time
sim_data <- simulate_data(1, 100)


f_m_prior <- brm(
  f, 
  data = sim_data,
  family = lognormal,
  prior = priors,
  sample_prior = 'only',
  backend = 'cmdstanr',
  cores = 3,
  control = list(
    adapt_delta = 0.99,
    max_treedepth = 20
  )
)
```
### Prior predictive check:
```{r}
summary(f_m_prior)
pp_check(f_m_prior, ndraws = 100)
```
### Fitting the model
```{r}
f_m <- brm(
  f, 
  data = sim_data,
  family = lognormal,
  prior = priors,
  sample_prior = TRUE,
  backend = 'cmdstanr',
  cores = 3,
  control = list(
    adapt_delta = 0.99,
    max_treedepth = 20
  )
)


fg_m <- brm(
  fg,
  data = sim_data,
  family = lognormal,
  prior = priors,
  sample_prior = T,
  backend = 'cmdstanr',
  cores = 3,
  control = list(
    adapt_delta = 0.99,
    max_treedepth = 20
  )
)
```
### Model quality checks:
```{r}
summary(f_m)
summary(fg_m)
```

#### Checking convergance:
```{r}
models <- list(f_m, fg_m)
      
# launch_shinystan(f_m) # - very nice for exploring and diagnosing the model, but opens up in a new window
map(.x = models, ~ mcmc_plot(.x, type = 'trace') + 
    theme_classic() + 
    scale_color_manual(values=c("#E66101", "#998EC3", "#542788", "#F1A340")) + 
    ylab("") + 
    xlab("Iteration") + 
    labs(subtitle = 'Trace Plots'))

map(.x = models, ~ mcmc_plot(.x, type = 'rhat_hist'))
map(.x = models, ~ mcmc_plot(.x, type = 'neff'))
```

#### Posterior predictive checks:
```{r}
pp_check(f_m, ndraws = 100)
pp_check(fg_m, ndraws = 100)
```




#### Prior-posterior updata checks:
```{r}
i <- get_variables(fg_m)
i[grepl("prior_sd", i, fixed = TRUE)]

```
#### Note:
For some reason, in the case of varying slopes by diagnosis the prior function seems to automatically only set the standard deviation priors for the 'ASD' diagnosis and not the 'TD' diagnosis.
From what we managed to find only the possibility of setting the priors separately for each group in gr(by = x) might be an open issue in brms (https://github.com/paul-buerkner/brms/issues/874). Because of that we only made prior - posterior plots for the 'ASD' group.
```{r}

pp_update_plot <- function(model){

df <- left_join(spread_draws(model, `.*b_.*`, regex = TRUE),
             spread_draws(model, `.*sd_.*`, regex = TRUE)) %>% 
  select(!c(.chain, .iteration, .draw) &
           !contains('cor') &
           !any_of(c("sd_id__Intercept:diagnosistd", "sd_id__visit:diagnosistd")))

gg_posteriors <- select(df, !starts_with('prior')) %>% as.list
gg_priors <- select(df, starts_with('prior')) %>% as.list
gg_titles <- names(gg_posteriors)

pmap(.l = list(.x = gg_priors, .y = gg_posteriors, .t = gg_titles),
     .f = function(.x, .y, .t){
       ggplot()+
            geom_density(aes(.x, fill = 'steelblue', alpha = 0.5))+
            geom_density(aes(.y, fill = '#FC4E07', alpha = 0.5))+
            ggtitle(.t)+
            theme_classic()+
            guides(fill = 'none', alpha = 'none') +
            labs(x = NULL)}
)}
```


```{r}
pp_update_plot(f_m, priors)
```


```{r}
pp_update_plot(fg_m, priors)
```
### Sensitivity analysis:
```{r}
sds <-  seq(0.1, 1, by = 0.1)

sensitivity_analysis<- function(sds, formula){

models_data <- map_df(
  .x = sds,
  .f = function(.x){
  
  new_priors <- priors
  new_priors[1, ] <- set_prior(paste0("normal(0, ", .x ,")"), class = "b")
    
  model <- brm(
    formula, 
    data = sim_data,
    family = lognormal,
    prior = new_priors,
    sample_prior = TRUE,
    backend = 'cmdstanr',
    cores = 3,
    control = list(
      adapt_delta = 0.99,
      max_treedepth = 20)
  )
  
  
    rbind(models_data,
          gather_draws(model, c(`b_diagnosisasd:visit`, `b_diagnosistd:visit`)) %>% median_qi(.width = 0.95))
})

write_rds(models_data, paste0(deparse('rdata/', substitute(formula)), "_sensitivity.Rds"))


map(.x = models_data %>% group_split(.variable),
    .f = ~ ggplot(data = .x, aes(x = sds, y = .value)) +
            geom_point(size = 3) +
            geom_pointrange(aes(ymin = .lower, ymax = .upper)) +
            ylim(0.1, 0.3) +
            labs(x = "Standard Deviation of Slope Prior", 
                 y = "Posterior Estimate for slope", 
                 title = paste0("'", .x$.variable, "'"),
                 subtitle = "Sensitivity analysis for multi-level model") +
            theme_bw() +
            theme(plot.title = element_text(hjust = 0.5, size = 15),
                  axis.title.x = element_text(size = 13),
                  axis.text.y = element_text(size = 12),
                  axis.text.x = element_text(size = 12),
                  axis.title.y = element_text(size = 13))

)}
```


```{r}
sensitivity_analysis(sds = sds, formula = f)
```


```{r}
sensitivity_analysis(sds = sds, formula = fg)
```
### Model exploration
#### Hypothesis testing
```{r}
conditional_effects(f_m)
conditional_effects(fg_m)
```
```{r}
hypothesis(f_m, 'diagnosistd:visit - diagnosisasd:visit > 0') 
hypothesis(fg_m, 'diagnosistd:visit - diagnosisasd:visit > 0') 
```



```{r}
# comparing model estimates with the true generative process values

head(p)


t_vs_est <- function(model){
est <- rbind(model %>% gather_draws(`b_.*`, regex = T) %>% mean_qi,
             model %>% gather_draws(`sd_.*`, regex = T) %>% mean_qi) %>% 
  select(!c(.variable, .point, .interval)) %>% 
  rename_with(.cols = everything(), ~ paste0("est_", .x) %>% str_remove_all(fixed('.'))) %>% 
  mutate(across(where(is.numeric), exp))

if (nrow(est) == 6){
  est <- rbind(est, est %>% slice(c(5,6)))
  } else
    est <- est

x <- rbind(est, est %>% slice(c(5,6)))

t_value <- p %>% ungroup %>% filter(!parameter == 'error')
t_value <- rbind(t_value %>% select(mu) %>% rename('t_value' = mu),
                 t_value %>% select(sd) %>% rename('t_value' = sd))

t_vs_est <- tibble(term = 
                    map2_chr(.x = rep(c('mu','sd'), each = 4),
                           .y = rep(c('intercept', 'slope'), times = 4),
                           .f = ~ paste0(.x, '_', .y)),
                   diagnosis = rep(c('asd', 'asd', 'td', 'td'), times = 2),
                   t_value,
                   est)

map(.x = t_vs_est %>% group_split(term),
    .f = ~ 
      ggplot(.x, aes(y = est_value, x = diagnosis, color = diagnosis)) +
        geom_hline(aes(yintercept = t_value, color = diagnosis)) +
        geom_point(size = 3) +
        geom_pointrange(aes(ymin = est_lower, ymax = est_upper)) +
        labs(title = paste0("'", .x$term, "'"),
             subtitle = "True value vs. model estimate",
             y = "True value",
             x = "Estimated value by diagnosis") +
        theme_bw() +
        theme(axis.text.x = element_blank())
    )
}
```


```{r}
t_vs_est(f_m)
```
```{r}
t_vs_est(fg_m)
```
#### individual effects

```{r}

ind_t_vs_est <- function(model){

t_value <- sim_data %>% ungroup %>% filter(visit == 1)
t_value <- rbind(
    t_value %>% select(ind_int) %>% rename('t_value' = ind_int),
    t_value %>% select(ind_slope) %>% rename('t_value' = ind_slope)
  )

ind_t_vs_est <- model %>% recover_types(sim_data) %>% gather_draws(r_id[id, term]) %>%
  mean_qi(.width = 0.95) %>% 
  select(!c(.variable, .point, .interval, .width)) %>% 
  rename_with(.cols = !c(term, id), ~ paste0("est_", .x) %>% str_remove_all(fixed('.')))%>% 
  mutate(across(where(is.numeric), exp)) %>% 
  cbind(t_value) %>% 
  mutate(accuracy = ifelse(t_value < est_upper & t_value > est_lower, 'accurate', 'inaccurate'),
         diagnosis = rep(c('asd', 'td'), each = 200)) 

map(.x = ind_t_vs_est %>% group_split(term),
    .f = ~
      ggplot(.x, aes(x = as.numeric(id), y = est_value, color = accuracy)) +
        geom_point(aes(shape = diagnosis), size = 3) +
        geom_pointrange(aes(ymin = est_lower, ymax = est_upper)) +
        scale_color_manual(values=c('green', 'red')) + 
        labs(title = 'True individual effects vs. model estimates',
             subtitle = paste0("'", .x$term, "'"),
             x = 'ID',
             y = 'Estimated value with 95% CI')
  )
}
        
```
```{r}
ind_t_vs_est(f_m)
```
```{r}
ind_t_vs_est(fg_m)
```
## Power Analysis
```{r}
update_fg <- function(n, seed){
  df <- 
    update(fg_m,
       newdata = simulate_data(n = n, seed = seed),
       seed = seed, 
       iter = 1000) %>% 
    gather_draws(c(`b_diagnosisasd:visit`,`b_diagnosistd:visit`)) %>%
    mean_qi(.width = 0.95) %>% 
    rename_with(.cols = everything(), ~ str_remove_all(.x, fixed('.'))) %>% 
    mutate(across(where(is.numeric), function(.x){ .x= lead(.x) - .x}),
           variable = 'b_diff') %>% 
    na.omit
}
#checking if it works
x <- update_fg(10,1)
xdiff <- update_fg(10,1) #after adding the last mutate
```


```{r}
power_data <- expand_grid(N = seq(10, 110, by = 20), seed = 1:100) %>% 
   mutate(dfs = update_fg(seed, N)) %>% 
   unnest(dfs)

write_rds(power_data, 'rdata/power_data.Rds')
```
### NHST type 'significance' approach:
```{r}
power_data %>% 
 ggplot(aes(x = seed %>% reorder(lower), y = value, ymin = lower, ymax = upper)) +
          geom_hline(yintercept = c(0, 0.2), color = "white") +
          geom_pointrange(fatten = 1/2) +
          labs(title = "Difference of mean slope parameters between the two groups",
               subtitle = paste0("N = ",  power_data$N),
               x = "seed (i.e., simulation index)",
               y = "value")+
          facet_grid(~ N)


power_data %>%
  mutate(signif = if_else(lower*upper > 0, 1, 0)) %>% 
  group_by(N) %>% 
  summarise(
    p_signif = mean(signif),
    se = sqrt(p_signif*(1 - p_signif) / n())
         ) %>% 
  mutate(lower.95 = ymin = p_signif - 1.96*se, 
         upper.95 = p_signif + 1.96*se) %>% 
  ggplot(aes(x = N, y = p_signif, ymin = lower.95, ymax = upper.95)) +
    geom_pointrange() +
    geom_line()
  
  
```
### Accuracy in parameter estimation (precision) approach :

```{r}
power_data %>% 
  group_by(N, seed) %>% 
  mutate(ci_width = upper - lower) %>% 
  ungroup %>% 
  ggplot(aes(x = ci_width)) %>% 
  geom_bar(binwidth = 0.1)


powerdata%>% 
  mutate(check = ifelse(ci_width < 0.7, 1, 0)) %>% 
  summarise(`proportion below 0.7` = mean(check),
            `average width` = mean(ci_width))


power_data %>%
  mutate(precise = if_else(width < 0.7, 1, 0)) %>% 
  group_by(N) %>% 
  summarise(
    p_precise = mean(precise),
    se = sqrt(p_precise*(1 - p_precise) / n())
         ) %>% 
  mutate(lower.95 = ymin = p_precise - 1.96*se, 
         upper.95 = p_precise + 1.96*se) %>% 
  ggplot(aes(x = N, y = p_precise, ymin = lower.95, ymax = upper.95)) +
    geom_pointrange() +
    geom_line()
  
```
### Conlusions:
```







## Conclusions